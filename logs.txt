root@4177bb297a48:~# git clone --single-branch --branch fast_tokenizers_BARTpho_PhoBERT_BERTweet https://github.com/datquocnguyen/transformers.git
Cloning into 'transformers'...
remote: Enumerating objects: 101243, done.
remote: Counting objects: 100% (47/47), done.
remote: Compressing objects: 100% (46/46), done.
remote: Total 101243 (delta 1), reused 47 (delta 1), pack-reused 101196
Receiving objects: 100% (101243/101243), 97.86 MiB | 12.35 MiB/s, done.
Resolving deltas: 100% (75249/75249), done.
root@4177bb297a48:~# cd transformers/
root@4177bb297a48:~/transformers# pip install -e .
Obtaining file:///root/transformers
  Installing build dependencies ... done
  Checking if build backend supports build_editable ... done
  Getting requirements to build editable ... done
  Preparing editable metadata (pyproject.toml) ... done
Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (21.3)
Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (2.28.1)
Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (4.12.0)
Collecting huggingface-hub<1.0,>=0.9.0
  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.5/163.5 kB 260.4 kB/s eta 0:00:00
Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (2021.11.10)
Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (4.64.0)
Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (6.0)
Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (3.7.1)
Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (1.21.6)
Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (0.12.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.9.0->transformers==4.23.0.dev0) (4.3.0)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.23.0.dev0) (3.0.9)
Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.23.0.dev0) (3.8.0)
Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.23.0.dev0) (2.1.0)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.23.0.dev0) (2022.6.15.2)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.23.0.dev0) (1.26.12)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.23.0.dev0) (3.3)
Building wheels for collected packages: transformers
  Building editable for transformers (pyproject.toml) ... done
  Created wheel for transformers: filename=transformers-4.23.0.dev0-0.editable-py3-none-any.whl size=29179 sha256=d18752bc5cdf6f934aaa6bb92bbf5ed99a651603fa830510bb8895d1088e8c33
  Stored in directory: /tmp/pip-ephem-wheel-cache-l3pd85x5/wheels/e6/e4/2d/4ae80a81be7bb59165b4aa68f1258bddcd87c9d2fc6863f4d4
Successfully built transformers
Installing collected packages: huggingface-hub, transformers
  Attempting uninstall: huggingface-hub
    Found existing installation: huggingface-hub 0.8.1
    Uninstalling huggingface-hub-0.8.1:
      Successfully uninstalled huggingface-hub-0.8.1
  Attempting uninstall: transformers
    Found existing installation: transformers 4.20.1
    Uninstalling transformers-4.20.1:
      Successfully uninstalled transformers-4.20.1
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
cached-path 1.1.5 requires huggingface-hub<0.9.0,>=0.8.1, but you have huggingface-hub 0.10.0 which is incompatible.
allennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.
allennlp 2.10.0 requires transformers<4.21,>=4.1, but you have transformers 4.23.0.dev0 which is incompatible.
Successfully installed huggingface-hub-0.10.0 transformers-4.23.0.dev0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
root@4177bb297a48:~/transformers# cd examples/pytorch/summarization/
root@4177bb297a48:~/transformers/examples/pytorch/summarization# pip install -r requirements.txt
Requirement already satisfied: accelerate in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (0.12.0)
Requirement already satisfied: datasets>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (2.1.0)
Requirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (0.1.97)
Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (3.19.4)
Collecting rouge-score
  Downloading rouge_score-0.1.2.tar.gz (17 kB)
  Preparing metadata (setup.py) ... done
Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (3.7)
Collecting py7zr
  Downloading py7zr-0.20.0-py3-none-any.whl (64 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.0/65.0 kB 256.5 kB/s eta 0:00:00
Requirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (1.11.0)
Collecting evaluate
  Downloading evaluate-0.2.2-py3-none-any.whl (69 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.8/69.8 kB 972.6 kB/s eta 0:00:00
Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from accelerate->-r requirements.txt (line 1)) (6.0)
Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from accelerate->-r requirements.txt (line 1)) (21.3)
Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from accelerate->-r requirements.txt (line 1)) (5.9.1)
Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from accelerate->-r requirements.txt (line 1)) (1.21.6)
Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.70.13)
Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.3.5.1)
Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.28.1)
Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.10.0)
Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.18.0)
Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (5.0.0)
Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.0.0)
Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2022.8.2)
Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.64.0)
Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.1)
Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.5)
Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.12.0)
Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from rouge-score->-r requirements.txt (line 5)) (0.15.0)
Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from rouge-score->-r requirements.txt (line 5)) (1.15.0)
Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->-r requirements.txt (line 6)) (1.0.1)
Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk->-r requirements.txt (line 6)) (2021.11.10)
Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->-r requirements.txt (line 6)) (8.0.4)
Collecting pyppmd<0.19.0,>=0.18.1
  Downloading pyppmd-0.18.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.6/138.6 kB 1.4 MB/s eta 0:00:00
Collecting brotli>=1.0.9
  Downloading Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 357.2/357.2 kB 1.6 MB/s eta 0:00:00
Collecting inflate64>=0.3.0
  Downloading inflate64-0.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.1/92.1 kB 4.3 MB/s eta 0:00:00
Collecting pyzstd>=0.14.4
  Downloading pyzstd-0.15.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (379 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 379.2/379.2 kB 5.5 MB/s eta 0:00:00
Collecting multivolumefile>=0.2.3
  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)
Requirement already satisfied: texttable in /opt/conda/lib/python3.7/site-packages (from py7zr->-r requirements.txt (line 7)) (1.6.4)
Collecting pycryptodomex>=3.6.6
  Downloading pycryptodomex-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 5.7 MB/s eta 0:00:00
Collecting pybcj>=0.6.0
  Downloading pybcj-1.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 5.2 MB/s eta 0:00:00
Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.3->-r requirements.txt (line 8)) (4.3.0)
Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.7.2)
Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.2)
Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.2.0)
Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (21.4.0)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.2)
Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (0.13.0)
Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (2.1.0)
Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.7.1)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->accelerate->-r requirements.txt (line 1)) (3.0.9)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.6.15.2)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (1.26.12)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.3)
Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.0)
Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)
Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.1)
Building wheels for collected packages: rouge-score
  Building wheel for rouge-score (setup.py) ... done
  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=c456abf0ed08029ad77d5682d1769f43f76a886640945b935ef129a73c3ad3c1
  Stored in directory: /root/.cache/pip/wheels/84/ac/6b/38096e3c5bf1dc87911e3585875e21a3ac610348e740409c76
Successfully built rouge-score
Installing collected packages: brotli, pyzstd, pyppmd, pycryptodomex, multivolumefile, pybcj, inflate64, py7zr, rouge-score, evaluate
Successfully installed brotli-1.0.9 evaluate-0.2.2 inflate64-0.3.0 multivolumefile-0.2.3 py7zr-0.20.0 pybcj-1.0.1 pycryptodomex-3.15.0 pyppmd-0.18.3 pyzstd-0.15.3 rouge-score-0.1.2
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
root@4177bb297a48:~/transformers/examples/pytorch/summarization# cd ../../..
root@4177bb297a48:~/transformers# cd ..
root@4177bb297a48:~# pip install git+https://github.com/huggingface/transformers
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-it9buqyi
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-it9buqyi
  Resolved https://github.com/huggingface/transformers to commit 01eb34ab45a8895fbd9e335568290e5d0f5f4491
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (0.10.0)
Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (1.21.6)
Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (0.12.1)
Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (2.28.1)
Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (3.7.1)
Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (4.12.0)
Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (4.64.0)
Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (21.3)
Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (2021.11.10)
Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.23.0.dev0) (6.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.23.0.dev0) (4.3.0)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.23.0.dev0) (3.0.9)
Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.23.0.dev0) (3.8.0)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.23.0.dev0) (2022.6.15.2)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.23.0.dev0) (1.26.12)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.23.0.dev0) (3.3)
Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.23.0.dev0) (2.1.0)
Building wheels for collected packages: transformers
  Building wheel for transformers (pyproject.toml) ... done
  Created wheel for transformers: filename=transformers-4.23.0.dev0-py3-none-any.whl size=5013752 sha256=1e61b2ab7e5d9bbf5b8fdef80087a2f51328820b826b0acf72b7fac7bac8f029
  Stored in directory: /tmp/pip-ephem-wheel-cache-t72wzygg/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522
Successfully built transformers
Installing collected packages: transformers
  Attempting uninstall: transformers
    Found existing installation: transformers 4.23.0.dev0
    Uninstalling transformers-4.23.0.dev0:
      Successfully uninstalled transformers-4.23.0.dev0
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
allennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.
allennlp 2.10.0 requires transformers<4.21,>=4.1, but you have transformers 4.23.0.dev0 which is incompatible.
Successfully installed transformers-4.23.0.dev0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
root@4177bb297a48:~# python transformers/examples/pytorch/summarization/run_summarization.py \
>     --model_name_or_path "vinai/bartpho-syllable" \
>     --do_train \
>     --do_eval \
>     --do_predict \
>     --tokenizer_name "vinai/bartpho-syllable" \
>     --num_train_epochs 10 \
>     --learning_rate 0.00001 \
>     --warmup_steps 20000 \
>     --train_file temp/train.json \
>     --validation_file temp/val.json \
>     --test_file temp/test.json \
>     --output_dir temp/tst-summarization \
>     --overwrite_output_dir \
>     --per_device_train_batch_size=4 \
>     --per_device_eval_batch_size=4 \
>     --predict_with_generate \
>     --evaluation_strategy epoch \
>     --save_strategy epoch \
>     --load_best_model_at_end \
>     --text_column text \
>     --summary_column summary 
Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-fdb2a07adb84496e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
Downloading data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 2946.13it/s]
Extracting data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1209.08it/s]
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/site-packages/datasets/packaged_modules/json/json.py", line 123, in _generate_tables
    io.BytesIO(batch), read_options=paj.ReadOptions(block_size=block_size)
  File "pyarrow/_json.pyx", line 246, in pyarrow._json.read_json
  File "pyarrow/error.pxi", line 143, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/error.pxi", line 99, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: JSON parse error: Column() changed from object to array in row 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "transformers/examples/pytorch/summarization/run_summarization.py", line 732, in <module>
    main()
  File "transformers/examples/pytorch/summarization/run_summarization.py", line 393, in main
    use_auth_token=True if model_args.use_auth_token else None,
  File "/opt/conda/lib/python3.7/site-packages/datasets/load.py", line 1696, in load_dataset
    use_auth_token=use_auth_token,
  File "/opt/conda/lib/python3.7/site-packages/datasets/builder.py", line 606, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File "/opt/conda/lib/python3.7/site-packages/datasets/builder.py", line 694, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/opt/conda/lib/python3.7/site-packages/datasets/builder.py", line 1152, in _prepare_split
    generator, unit=" tables", leave=False, disable=True  # not logging.is_progress_bar_enabled()
  File "/opt/conda/lib/python3.7/site-packages/tqdm/std.py", line 1183, in __iter__
    for obj in iterable:
  File "/opt/conda/lib/python3.7/site-packages/datasets/packaged_modules/json/json.py", line 148, in _generate_tables
    f"Not able to read records in the JSON file at {file}. "
AttributeError: 'list' object has no attribute 'keys'
root@4177bb297a48:~# python transformers/examples/pytorch/summarization/run_summarization.py     --model_name_or_path "vinai/bartpho-syllable"     --do_train     --do_eval     --do_predict     --tokenizer_name "vinai/bartpho-syllable"     --num_train_epochs 10     --learning_rate 0.00001     --warmup_steps 20000     --train_file temp/train.json     --validation_file temp/val.json     --test_file temp/test.json     --output_dir temp/tst-summarization     --overwrite_output_dir     --per_device_train_batch_size=4     --per_device_eval_batch_size=4     --predict_with_generate     --evaluation_strategy epoch     --save_strategy epoch     --load_best_model_at_end     --text_column text     --summary_column summary 
{'train': 'temp/train.json', 'validation': 'temp/val.json', 'test': 'temp/test.json'}
Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-fdb2a07adb84496e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 7810.62it/s]
Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 958.92it/s]
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/site-packages/datasets/packaged_modules/json/json.py", line 123, in _generate_tables
    io.BytesIO(batch), read_options=paj.ReadOptions(block_size=block_size)
  File "pyarrow/_json.pyx", line 246, in pyarrow._json.read_json
  File "pyarrow/error.pxi", line 143, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/error.pxi", line 99, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: JSON parse error: Column() changed from object to array in row 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "transformers/examples/pytorch/summarization/run_summarization.py", line 733, in <module>
    main()
  File "transformers/examples/pytorch/summarization/run_summarization.py", line 394, in main
    use_auth_token=True if model_args.use_auth_token else None,
  File "/opt/conda/lib/python3.7/site-packages/datasets/load.py", line 1696, in load_dataset
    use_auth_token=use_auth_token,
  File "/opt/conda/lib/python3.7/site-packages/datasets/builder.py", line 606, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File "/opt/conda/lib/python3.7/site-packages/datasets/builder.py", line 694, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/opt/conda/lib/python3.7/site-packages/datasets/builder.py", line 1152, in _prepare_split
    generator, unit=" tables", leave=False, disable=True  # not logging.is_progress_bar_enabled()
  File "/opt/conda/lib/python3.7/site-packages/tqdm/std.py", line 1183, in __iter__
    for obj in iterable:
  File "/opt/conda/lib/python3.7/site-packages/datasets/packaged_modules/json/json.py", line 148, in _generate_tables
    f"Not able to read records in the JSON file at {file}. "
AttributeError: 'list' object has no attribute 'keys'
root@4177bb297a48:~# ^C
root@4177bb297a48:~# rm -r temp/
root@4177bb297a48:~# git clone https://github.com/lemanhha/temp.git
Cloning into 'temp'...
remote: Enumerating objects: 10, done.
remote: Counting objects: 100% (10/10), done.
remote: Compressing objects: 100% (7/7), done.
remote: Total 10 (delta 3), reused 8 (delta 3), pack-reused 0
Unpacking objects: 100% (10/10), 684.28 KiB | 634.00 KiB/s, done.
root@4177bb297a48:~# python transformers/examples/pytorch/summarization/run_summarization.py     --model_name_or_path "vinai/bartpho-syllable"     --do_train     --do_eval     --do_predict     --tokenizer_name "vinai/bartpho-syllable"     --num_train_epochs 10     --learning_rate 0.00001     --warmup_steps 20000     --train_file temp/train.json     --validation_file temp/val.json     --test_file temp/test.json     --output_dir temp/tst-summarization     --overwrite_output_dir     --per_device_train_batch_size=4     --per_device_eval_batch_size=4     --predict_with_generate     --evaluation_strategy epoch     --save_strategy epoch     --load_best_model_at_end     --text_column text     --summary_column summary 
{'train': 'temp/train.json', 'validation': 'temp/val.json', 'test': 'temp/test.json'}
Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-01e66cf6e61181e9/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 4721.54it/s]
Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1260.81it/s]
Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-01e66cf6e61181e9/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 782.96it/s]
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 897/897 [00:00<00:00, 717kB/s]
[INFO|configuration_utils.py:651] 2022-09-29 16:10:54,057 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--bartpho-syllable/snapshots/b32e9f30ae00d1907d5a20c1d1786575d7db8e61/config.json
[INFO|configuration_utils.py:703] 2022-09-29 16:10:54,061 >> Model config MBartConfig {
  "_name_or_path": "vinai/bartpho-syllable",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "MBartModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "max_position_embeddings": 1024,
  "model_type": "mbart",
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "scale_embedding": false,
  "tokenizer_class": "BartphoTokenizer",
  "torch_dtype": "float32",
  "transformers_version": "4.23.0.dev0",
  "use_cache": true,
  "vocab_size": 40030
}

[INFO|tokenization_auto.py:417] 2022-09-29 16:10:55,079 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:651] 2022-09-29 16:10:56,090 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--bartpho-syllable/snapshots/b32e9f30ae00d1907d5a20c1d1786575d7db8e61/config.json
[INFO|configuration_utils.py:703] 2022-09-29 16:10:56,091 >> Model config MBartConfig {
  "_name_or_path": "vinai/bartpho-syllable",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "MBartModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "max_position_embeddings": 1024,
  "model_type": "mbart",
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "scale_embedding": false,
  "tokenizer_class": "BartphoTokenizer",
  "torch_dtype": "float32",
  "transformers_version": "4.23.0.dev0",
  "use_cache": true,
  "vocab_size": 40030
}

Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.07M/5.07M [00:01<00:00, 3.73MB/s]
Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 360k/360k [00:00<00:00, 450kB/s]
[INFO|tokenization_utils_base.py:1773] 2022-09-29 16:11:04,237 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--vinai--bartpho-syllable/snapshots/b32e9f30ae00d1907d5a20c1d1786575d7db8e61/sentencepiece.bpe.model
[INFO|tokenization_utils_base.py:1773] 2022-09-29 16:11:04,238 >> loading file dict.txt from cache at /root/.cache/huggingface/hub/models--vinai--bartpho-syllable/snapshots/b32e9f30ae00d1907d5a20c1d1786575d7db8e61/dict.txt
[INFO|tokenization_utils_base.py:1773] 2022-09-29 16:11:04,238 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1773] 2022-09-29 16:11:04,238 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1773] 2022-09-29 16:11:04,238 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:651] 2022-09-29 16:11:04,239 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--bartpho-syllable/snapshots/b32e9f30ae00d1907d5a20c1d1786575d7db8e61/config.json
[INFO|configuration_utils.py:703] 2022-09-29 16:11:04,240 >> Model config MBartConfig {
  "_name_or_path": "vinai/bartpho-syllable",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "MBartModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "max_position_embeddings": 1024,
  "model_type": "mbart",
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "scale_embedding": false,
  "tokenizer_class": "BartphoTokenizer",
  "torch_dtype": "float32",
  "transformers_version": "4.23.0.dev0",
  "use_cache": true,
  "vocab_size": 40030
}

Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.58G/1.58G [01:39<00:00, 15.9MB/s]
[INFO|modeling_utils.py:2088] 2022-09-29 16:12:46,109 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--bartpho-syllable/snapshots/b32e9f30ae00d1907d5a20c1d1786575d7db8e61/pytorch_model.bin
[INFO|modeling_utils.py:2538] 2022-09-29 16:12:52,287 >> All model checkpoint weights were used when initializing MBartForConditionalGeneration.

[INFO|modeling_utils.py:2547] 2022-09-29 16:12:52,287 >> All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at vinai/bartpho-syllable.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.
Running tokenizer on train dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.46s/ba]
Running tokenizer on validation dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.84ba/s]
Running tokenizer on prediction dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.11ba/s]
Downloading builder script: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6.27k/6.27k [00:00<00:00, 4.47MB/s]
/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
[INFO|trainer.py:1607] 2022-09-29 16:13:02,096 >> ***** Running training *****
[INFO|trainer.py:1608] 2022-09-29 16:13:02,096 >>   Num examples = 448
[INFO|trainer.py:1609] 2022-09-29 16:13:02,096 >>   Num Epochs = 10
[INFO|trainer.py:1610] 2022-09-29 16:13:02,096 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:1611] 2022-09-29 16:13:02,096 >>   Total train batch size (w. parallel, distributed & accumulation) = 4
[INFO|trainer.py:1612] 2022-09-29 16:13:02,096 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1613] 2022-09-29 16:13:02,096 >>   Total optimization steps = 1120
[INFO|integrations.py:681] 2022-09-29 16:13:02,114 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 1
wandb: You chose 'Create a W&B account'
wandb: Create an account here: https://wandb.ai/authorize?signup=true
wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: 
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py", line 1040, in init
    wi.setup(kwargs)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py", line 264, in setup
    _entity=kwargs.get("entity") or settings.entity,
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_login.py", line 297, in _login
    wlogin.prompt_api_key()
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_login.py", line 220, in prompt_api_key
    key, status = self._prompt_api_key()
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_login.py", line 204, in _prompt_api_key
    no_create=self._settings.force if self._settings else None,
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/apikey.py", line 115, in prompt_api_key
    key = input_callback(api_ask).strip()
  File "/opt/conda/lib/python3.7/site-packages/click/termui.py", line 166, in prompt
    value = prompt_func(prompt)
  File "/opt/conda/lib/python3.7/site-packages/click/termui.py", line 149, in prompt_func
    raise Abort() from None
click.exceptions.Abort
wandb: ERROR Abnormal program exit
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py", line 1040, in init
    wi.setup(kwargs)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py", line 264, in setup
    _entity=kwargs.get("entity") or settings.entity,
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_login.py", line 297, in _login
    wlogin.prompt_api_key()
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_login.py", line 220, in prompt_api_key
    key, status = self._prompt_api_key()
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_login.py", line 204, in _prompt_api_key
    no_create=self._settings.force if self._settings else None,
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/apikey.py", line 115, in prompt_api_key
    key = input_callback(api_ask).strip()
  File "/opt/conda/lib/python3.7/site-packages/click/termui.py", line 166, in prompt
    value = prompt_func(prompt)
  File "/opt/conda/lib/python3.7/site-packages/click/termui.py", line 149, in prompt_func
    raise Abort() from None
click.exceptions.Abort

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "transformers/examples/pytorch/summarization/run_summarization.py", line 733, in <module>
    main()
  File "transformers/examples/pytorch/summarization/run_summarization.py", line 652, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py", line 1504, in train
    ignore_keys_for_eval=ignore_keys_for_eval,
  File "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py", line 1671, in _inner_training_loop
    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)
  File "/opt/conda/lib/python3.7/site-packages/transformers/trainer_callback.py", line 353, in on_train_begin
    return self.call_event("on_train_begin", args, state, control)
  File "/opt/conda/lib/python3.7/site-packages/transformers/trainer_callback.py", line 407, in call_event
    **kwargs,
  File "/opt/conda/lib/python3.7/site-packages/transformers/integrations.py", line 725, in on_train_begin
    self.setup(args, state, model, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/transformers/integrations.py", line 700, in setup
    **init_args,
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py", line 1081, in init
    raise Exception("problem") from error_seen
Exception: problem
root@4177bb297a48:~# python transformers/examples/pytorch/summarization/run_summarization.py     --model_name_or_path "vinai/bartpho-syllable"     --do_train     --do_eval     --do_predict     --tokenizer_name "vinai/bartpho-syllable"     --num_train_epochs 10     --learning_rate 0.00001     --warmup_steps 20000     --train_file temp/train.json     --validation_file temp/val.json     --test_file temp/test.json     --output_dir temp/tst-summarization     --overwrite_output_dir     --per_device_train_batch_size=4     --per_device_eval_batch_size=4     --predict_with_generate     --evaluation_strategy epoch     --save_strategy epoch     --load_best_model_at_end     --text_column text     --summary_column summary 
The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
Moving 0 files to the new cache system
0it [00:00, ?it/s]
{'train': 'temp/train.json', 'validation': 'temp/val.json', 'test': 'temp/test.json'}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 665.80it/s]
[INFO|configuration_utils.py:651] 2022-09-29 16:14:25,452 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--bartpho-syllable/snapshots/b32e9f30ae00d1907d5a20c1d1786575d7db8e61/config.json
[INFO|configuration_utils.py:703] 2022-09-29 16:14:25,456 >> Model config MBartConfig {
  "_name_or_path": "vinai/bartpho-syllable",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "MBartModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "max_position_embeddings": 1024,
  "model_type": "mbart",
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "scale_embedding": false,
  "tokenizer_class": "BartphoTokenizer",
  "torch_dtype": "float32",
  "transformers_version": "4.23.0.dev0",
  "use_cache": true,
  "vocab_size": 40030
}

[INFO|tokenization_auto.py:417] 2022-09-29 16:14:26,465 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:651] 2022-09-29 16:14:27,476 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--bartpho-syllable/snapshots/b32e9f30ae00d1907d5a20c1d1786575d7db8e61/config.json
[INFO|configuration_utils.py:703] 2022-09-29 16:14:27,478 >> Model config MBartConfig {
  "_name_or_path": "vinai/bartpho-syllable",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "MBartModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "max_position_embeddings": 1024,
  "model_type": "mbart",
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "scale_embedding": false,
  "tokenizer_class": "BartphoTokenizer",
  "torch_dtype": "float32",
  "transformers_version": "4.23.0.dev0",
  "use_cache": true,
  "vocab_size": 40030
}

[INFO|tokenization_utils_base.py:1773] 2022-09-29 16:14:28,492 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--vinai--bartpho-syllable/snapshots/b32e9f30ae00d1907d5a20c1d1786575d7db8e61/sentencepiece.bpe.model
[INFO|tokenization_utils_base.py:1773] 2022-09-29 16:14:28,492 >> loading file dict.txt from cache at /root/.cache/huggingface/hub/models--vinai--bartpho-syllable/snapshots/b32e9f30ae00d1907d5a20c1d1786575d7db8e61/dict.txt
[INFO|tokenization_utils_base.py:1773] 2022-09-29 16:14:28,492 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1773] 2022-09-29 16:14:28,493 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1773] 2022-09-29 16:14:28,493 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:651] 2022-09-29 16:14:28,493 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--bartpho-syllable/snapshots/b32e9f30ae00d1907d5a20c1d1786575d7db8e61/config.json
[INFO|configuration_utils.py:703] 2022-09-29 16:14:28,495 >> Model config MBartConfig {
  "_name_or_path": "vinai/bartpho-syllable",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "MBartModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "max_position_embeddings": 1024,
  "model_type": "mbart",
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "scale_embedding": false,
  "tokenizer_class": "BartphoTokenizer",
  "torch_dtype": "float32",
  "transformers_version": "4.23.0.dev0",
  "use_cache": true,
  "vocab_size": 40030
}

[INFO|modeling_utils.py:2088] 2022-09-29 16:14:29,117 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--bartpho-syllable/snapshots/b32e9f30ae00d1907d5a20c1d1786575d7db8e61/pytorch_model.bin
[INFO|modeling_utils.py:2538] 2022-09-29 16:14:34,832 >> All model checkpoint weights were used when initializing MBartForConditionalGeneration.

[INFO|modeling_utils.py:2547] 2022-09-29 16:14:34,832 >> All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at vinai/bartpho-syllable.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.
/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
[INFO|trainer.py:1607] 2022-09-29 16:14:39,123 >> ***** Running training *****
[INFO|trainer.py:1608] 2022-09-29 16:14:39,123 >>   Num examples = 448
[INFO|trainer.py:1609] 2022-09-29 16:14:39,123 >>   Num Epochs = 10
[INFO|trainer.py:1610] 2022-09-29 16:14:39,123 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:1611] 2022-09-29 16:14:39,123 >>   Total train batch size (w. parallel, distributed & accumulation) = 4
[INFO|trainer.py:1612] 2022-09-29 16:14:39,123 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1613] 2022-09-29 16:14:39,123 >>   Total optimization steps = 1120
[INFO|integrations.py:681] 2022-09-29 16:14:39,142 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 3
wandb: You chose 'Don't visualize my results'
wandb: Tracking run with wandb version 0.12.21
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
 10%|██████████████▏                                                                                                                               | 112/1120 [01:55<17:03,  1.02s/it][INFO|trainer.py:2907] 2022-09-29 16:17:52,519 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2022-09-29 16:17:52,519 >>   Num examples = 85
[INFO|trainer.py:2912] 2022-09-29 16:17:52,519 >>   Batch size = 4
                                                                                                                                                                                      
{'eval_loss': 2.7335331439971924, 'eval_rouge1': 20.0299, 'eval_rouge2': 6.4034, 'eval_rougeL': 15.3818, 'eval_rougeLsum': 15.6424, 'eval_gen_len': 20.0, 'eval_runtime': 17.6999, 'ev 10%|██████████████▏                                                                                                                               | 112/1120 [02:13<17:03,  1.02s/it[INFO|trainer.py:2656] 2022-09-29 16:18:10,225 >> Saving model checkpoint to temp/tst-summarization/checkpoint-112                                                                     
[INFO|configuration_utils.py:445] 2022-09-29 16:18:10,227 >> Configuration saved in temp/tst-summarization/checkpoint-112/config.json
[INFO|modeling_utils.py:1583] 2022-09-29 16:18:14,205 >> Model weights saved in temp/tst-summarization/checkpoint-112/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-09-29 16:18:14,275 >> tokenizer config file saved in temp/tst-summarization/checkpoint-112/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-09-29 16:18:14,276 >> Special tokens file saved in temp/tst-summarization/checkpoint-112/special_tokens_map.json
 20%|████████████████████████████▍                                                                                                                 | 224/1120 [04:24<15:18,  1.03s/it][INFO|trainer.py:2907] 2022-09-29 16:20:21,182 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2022-09-29 16:20:21,183 >>   Num examples = 85
[INFO|trainer.py:2912] 2022-09-29 16:20:21,183 >>   Batch size = 4
{'eval_loss': 2.5974225997924805, 'eval_rouge1': 19.3265, 'eval_rouge2': 5.8998, 'eval_rougeL': 14.9048, 'eval_rougeLsum': 15.0299, 'eval_gen_len': 20.0, 'eval_runtime': 16.3975, 'eval_samples_per_second': 5.184, 'eval_steps_per_second': 1.342, 'epoch': 2.0}                                                                                                          
 20%|████████████████████████████▍                                                                                                                 | 224/1120 [04:40<15:18,  1.03s/it[INFO|trainer.py:2656] 2022-09-29 16:20:37,585 >> Saving model checkpoint to temp/tst-summarization/checkpoint-224                                                                     
[INFO|configuration_utils.py:445] 2022-09-29 16:20:37,588 >> Configuration saved in temp/tst-summarization/checkpoint-224/config.json
[INFO|modeling_utils.py:1583] 2022-09-29 16:20:41,248 >> Model weights saved in temp/tst-summarization/checkpoint-224/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-09-29 16:20:41,322 >> tokenizer config file saved in temp/tst-summarization/checkpoint-224/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-09-29 16:20:41,323 >> Special tokens file saved in temp/tst-summarization/checkpoint-224/special_tokens_map.json
 30%|██████████████████████████████████████████▌                                                                                                   | 336/1120 [06:51<13:35,  1.04s/it][INFO|trainer.py:2907] 2022-09-29 16:22:48,271 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2022-09-29 16:22:48,271 >>   Num examples = 85
[INFO|trainer.py:2912] 2022-09-29 16:22:48,271 >>   Batch size = 4
{'eval_loss': 2.417017936706543, 'eval_rouge1': 18.8756, 'eval_rouge2': 6.0328, 'eval_rougeL': 14.4985, 'eval_rougeLsum': 14.8216, 'eval_gen_len': 20.0, 'eval_runtime': 16.4532, 'eval_samples_per_second': 5.166, 'eval_steps_per_second': 1.337, 'epoch': 3.0}                                                                                                           
 30%|██████████████████████████████████████████▌                                                                                                   | 336/1120 [07:07<13:35,  1.04s/it[INFO|trainer.py:2656] 2022-09-29 16:23:04,731 >> Saving model checkpoint to temp/tst-summarization/checkpoint-336                                                                     
[INFO|configuration_utils.py:445] 2022-09-29 16:23:04,733 >> Configuration saved in temp/tst-summarization/checkpoint-336/config.json
[INFO|modeling_utils.py:1583] 2022-09-29 16:23:08,949 >> Model weights saved in temp/tst-summarization/checkpoint-336/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-09-29 16:23:08,952 >> tokenizer config file saved in temp/tst-summarization/checkpoint-336/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-09-29 16:23:08,953 >> Special tokens file saved in temp/tst-summarization/checkpoint-336/special_tokens_map.json
 40%|████████████████████████████████████████████████████████▊                                                                                     | 448/1120 [09:18<11:01,  1.02it/s][INFO|trainer.py:2907] 2022-09-29 16:25:15,620 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2022-09-29 16:25:15,621 >>   Num examples = 85
[INFO|trainer.py:2912] 2022-09-29 16:25:15,621 >>   Batch size = 4
{'eval_loss': 2.2559735774993896, 'eval_rouge1': 18.6502, 'eval_rouge2': 6.0007, 'eval_rougeL': 14.2747, 'eval_rougeLsum': 14.6339, 'eval_gen_len': 20.0, 'eval_runtime': 16.5167, 'eval_samples_per_second': 5.146, 'eval_steps_per_second': 1.332, 'epoch': 4.0}██████████████████████████████████████████████████████████████████████████| 22/22 [00:15<00:00,  1.53it/s]
 40%|████████████████████████████████████████████████████████▊                                                                                     | 448/1120 [09:35<11:01,  1.02it/s[INFO|trainer.py:2656] 2022-09-29 16:25:32,141 >> Saving model checkpoint to temp/tst-summarization/checkpoint-448                                                                     
[INFO|configuration_utils.py:445] 2022-09-29 16:25:32,142 >> Configuration saved in temp/tst-summarization/checkpoint-448/config.json
[INFO|modeling_utils.py:1583] 2022-09-29 16:25:35,895 >> Model weights saved in temp/tst-summarization/checkpoint-448/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-09-29 16:25:35,897 >> tokenizer config file saved in temp/tst-summarization/checkpoint-448/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-09-29 16:25:35,898 >> Special tokens file saved in temp/tst-summarization/checkpoint-448/special_tokens_map.json
{'loss': 4.9018, 'learning_rate': 2.5000000000000004e-07, 'epoch': 4.46}                                                                                                              
 50%|███████████████████████████████████████████████████████████████████████                                                                       | 560/1120 [11:46<09:09,  1.02it/s][INFO|trainer.py:2907] 2022-09-29 16:27:43,431 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2022-09-29 16:27:43,431 >>   Num examples = 85
[INFO|trainer.py:2912] 2022-09-29 16:27:43,431 >>   Batch size = 4
{'eval_loss': 2.1343555450439453, 'eval_rouge1': 17.2679, 'eval_rouge2': 4.9546, 'eval_rougeL': 13.0845, 'eval_rougeLsum': 13.2462, 'eval_gen_len': 20.0, 'eval_runtime': 16.3431, 'eval_samples_per_second': 5.201, 'eval_steps_per_second': 1.346, 'epoch': 5.0}                                                                                                          
 50%|███████████████████████████████████████████████████████████████████████                                                                       | 560/1120 [12:02<09:09,  1.02it/s[INFO|trainer.py:2656] 2022-09-29 16:27:59,783 >> Saving model checkpoint to temp/tst-summarization/checkpoint-560                                                                     
[INFO|configuration_utils.py:445] 2022-09-29 16:27:59,785 >> Configuration saved in temp/tst-summarization/checkpoint-560/config.json
[INFO|modeling_utils.py:1583] 2022-09-29 16:28:04,339 >> Model weights saved in temp/tst-summarization/checkpoint-560/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-09-29 16:28:04,341 >> tokenizer config file saved in temp/tst-summarization/checkpoint-560/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-09-29 16:28:04,341 >> Special tokens file saved in temp/tst-summarization/checkpoint-560/special_tokens_map.json
 60%|█████████████████████████████████████████████████████████████████████████████████████▏                                                        | 672/1120 [14:15<07:30,  1.00s/it][INFO|trainer.py:2907] 2022-09-29 16:30:12,950 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2022-09-29 16:30:12,950 >>   Num examples = 85
[INFO|trainer.py:2912] 2022-09-29 16:30:12,950 >>   Batch size = 4
                                                                                                                                                                                      {'eval_loss': 2.0144147872924805, 'eval_rouge1': 15.5183, 'eval_rouge2': 4.9521, 'eval_rougeL': 12.2315, 'eval_rougeLsum': 12.2813, 'eval_gen_len': 20.0, 'eval_runtime': 16.7141, 'eval_samples_per_second': 5.086, 'eval_steps_per_second': 1.316, 'epoch': 6.0}
 60%|█████████████████████████████████████████████████████████████████████████████████████▏                                                        | 672/1120 [14:32<07:30,  1.00s/it[INFO|trainer.py:2656] 2022-09-29 16:30:29,667 >> Saving model checkpoint to temp/tst-summarization/checkpoint-672                                                                     
[INFO|configuration_utils.py:445] 2022-09-29 16:30:29,669 >> Configuration saved in temp/tst-summarization/checkpoint-672/config.json
[INFO|modeling_utils.py:1583] 2022-09-29 16:30:33,746 >> Model weights saved in temp/tst-summarization/checkpoint-672/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-09-29 16:30:33,748 >> tokenizer config file saved in temp/tst-summarization/checkpoint-672/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-09-29 16:30:33,748 >> Special tokens file saved in temp/tst-summarization/checkpoint-672/special_tokens_map.json
 70%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                          | 784/1120 [16:43<05:42,  1.02s/it][INFO|trainer.py:2907] 2022-09-29 16:32:40,915 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2022-09-29 16:32:40,915 >>   Num examples = 85
[INFO|trainer.py:2912] 2022-09-29 16:32:40,915 >>   Batch size = 4
                                                                                                                                                                                     {'eval_loss': 1.9161750078201294, 'eval_rouge1': 16.8767, 'eval_rouge2': 6.025, 'eval_rougeL': 13.4611, 'eval_rougeLsum': 13.6226, 'eval_gen_len': 20.0, 'eval_runtime': 16.3011, 'eval_samples_per_second': 5.214, 'eval_steps_per_second': 1.35, 'epoch': 7.0}
 70%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                          | 784/1120 [17:00<05:42,  1.02s/it[INFO|trainer.py:2656] 2022-09-29 16:32:57,219 >> Saving model checkpoint to temp/tst-summarization/checkpoint-784                                                                     
[INFO|configuration_utils.py:445] 2022-09-29 16:32:57,220 >> Configuration saved in temp/tst-summarization/checkpoint-784/config.json
[INFO|modeling_utils.py:1583] 2022-09-29 16:33:01,661 >> Model weights saved in temp/tst-summarization/checkpoint-784/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-09-29 16:33:01,665 >> tokenizer config file saved in temp/tst-summarization/checkpoint-784/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-09-29 16:33:01,665 >> Special tokens file saved in temp/tst-summarization/checkpoint-784/special_tokens_map.json
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                            | 896/1120 [19:10<03:42,  1.01it/s][INFO|trainer.py:2907] 2022-09-29 16:35:07,509 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2022-09-29 16:35:07,509 >>   Num examples = 85
[INFO|trainer.py:2912] 2022-09-29 16:35:07,509 >>   Batch size = 4
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                            | 896/1120 [19:27<03:42,  1.01it/s{'eval_loss': 1.8440665006637573, 'eval_rouge1': 16.6016, 'eval_rouge2': 5.8238, 'eval_rougeL': 13.8107, 'eval_rougeLsum': 13.9046, 'eval_gen_len': 20.0, 'eval_runtime': 16.5667, 'eval_samples_per_second': 5.131, 'eval_steps_per_second': 1.328, 'epoch': 8.0}
[INFO|trainer.py:2656] 2022-09-29 16:35:24,079 >> Saving model checkpoint to temp/tst-summarization/checkpoint-896
[INFO|configuration_utils.py:445] 2022-09-29 16:35:24,084 >> Configuration saved in temp/tst-summarization/checkpoint-896/config.json
[INFO|modeling_utils.py:1583] 2022-09-29 16:35:28,150 >> Model weights saved in temp/tst-summarization/checkpoint-896/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-09-29 16:35:28,152 >> tokenizer config file saved in temp/tst-summarization/checkpoint-896/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-09-29 16:35:28,153 >> Special tokens file saved in temp/tst-summarization/checkpoint-896/special_tokens_map.json
{'loss': 3.1809, 'learning_rate': 5.000000000000001e-07, 'epoch': 8.93}                                                                                                               
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉              | 1008/1120 [21:39<01:48,  1.03it/s][INFO|trainer.py:2907] 2022-09-29 16:37:36,296 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2022-09-29 16:37:36,296 >>   Num examples = 85
[INFO|trainer.py:2912] 2022-09-29 16:37:36,296 >>   Batch size = 4
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉              | 1008/1120 [21:55<01:48,  1.03it/s{'eval_loss': 1.8042830228805542, 'eval_rouge1': 16.6981, 'eval_rouge2': 5.8265, 'eval_rougeL': 13.6528, 'eval_rougeLsum': 13.9474, 'eval_gen_len': 20.0, 'eval_runtime': 16.2511, 'eval_samples_per_second': 5.23, 'eval_steps_per_second': 1.354, 'epoch': 9.0}
[INFO|trainer.py:2656] 2022-09-29 16:37:52,550 >> Saving model checkpoint to temp/tst-summarization/checkpoint-1008
[INFO|configuration_utils.py:445] 2022-09-29 16:37:52,552 >> Configuration saved in temp/tst-summarization/checkpoint-1008/config.json
[INFO|modeling_utils.py:1583] 2022-09-29 16:37:59,016 >> Model weights saved in temp/tst-summarization/checkpoint-1008/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-09-29 16:37:59,098 >> tokenizer config file saved in temp/tst-summarization/checkpoint-1008/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-09-29 16:37:59,099 >> Special tokens file saved in temp/tst-summarization/checkpoint-1008/special_tokens_map.json
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1120/1120 [24:08<00:00,  1.08it/s][INFO|trainer.py:2907] 2022-09-29 16:40:05,171 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2022-09-29 16:40:05,172 >>   Num examples = 85
[INFO|trainer.py:2912] 2022-09-29 16:40:05,173 >>   Batch size = 4
{'eval_loss': 1.7651888132095337, 'eval_rouge1': 16.5368, 'eval_rouge2': 6.1053, 'eval_rougeL': 14.1467, 'eval_rougeLsum': 14.314, 'eval_gen_len': 20.0, 'eval_runtime': 16.2824, 'eval_samples_per_second': 5.22, 'eval_steps_per_second': 1.351, 'epoch': 10.0}                                                                                                           
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1120/1120 [24:24<00:00,  1.08it/s[INFO|trainer.py:2656] 2022-09-29 16:40:21,458 >> Saving model checkpoint to temp/tst-summarization/checkpoint-1120                                                                    
[INFO|configuration_utils.py:445] 2022-09-29 16:40:21,459 >> Configuration saved in temp/tst-summarization/checkpoint-1120/config.json
[INFO|modeling_utils.py:1583] 2022-09-29 16:40:25,617 >> Model weights saved in temp/tst-summarization/checkpoint-1120/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-09-29 16:40:25,619 >> tokenizer config file saved in temp/tst-summarization/checkpoint-1120/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-09-29 16:40:25,621 >> Special tokens file saved in temp/tst-summarization/checkpoint-1120/special_tokens_map.json
[INFO|trainer.py:1852] 2022-09-29 16:40:41,506 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1946] 2022-09-29 16:40:41,506 >> Loading best model from temp/tst-summarization/checkpoint-1120 (score: 1.7651888132095337).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1120/1120 [24:53<00:00,  1.08it/s]{'train_runtime': 1571.1378, 'train_samples_per_second': 2.851, 'train_steps_per_second': 0.713, 'train_loss': 3.8925916399274554, 'epoch': 10.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1120/1120 [24:53<00:00,  1.33s/it]
[INFO|trainer.py:2656] 2022-09-29 16:40:50,267 >> Saving model checkpoint to temp/tst-summarization
[INFO|configuration_utils.py:445] 2022-09-29 16:40:50,283 >> Configuration saved in temp/tst-summarization/config.json
[INFO|modeling_utils.py:1583] 2022-09-29 16:40:55,790 >> Model weights saved in temp/tst-summarization/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-09-29 16:40:55,792 >> tokenizer config file saved in temp/tst-summarization/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-09-29 16:40:55,792 >> Special tokens file saved in temp/tst-summarization/special_tokens_map.json
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     3.8926
  train_runtime            = 0:26:11.13
  train_samples            =        448
  train_samples_per_second =      2.851
  train_steps_per_second   =      0.713
[INFO|trainer.py:2907] 2022-09-29 16:40:55,818 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2022-09-29 16:40:55,819 >>   Num examples = 85
[INFO|trainer.py:2912] 2022-09-29 16:40:55,819 >>   Batch size = 4
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:36<00:00,  1.61s/it]***** eval metrics *****
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:37<00:00,  1.71s/it]  epoch                   =       10.0
  eval_gen_len            =   115.1529

  eval_loss               =     1.7652
  eval_rouge1             =    44.8148
  eval_rouge2             =    17.7844
  eval_rougeL             =    28.0966
  eval_rougeLsum          =    30.9733
  eval_runtime            = 0:00:39.45
  eval_samples            =         85
  eval_samples_per_second =      2.155
  eval_steps_per_second   =      0.558
[INFO|trainer.py:2907] 2022-09-29 16:41:35,283 >> ***** Running Prediction *****
[INFO|trainer.py:2909] 2022-09-29 16:41:35,283 >>   Num examples = 95
[INFO|trainer.py:2912] 2022-09-29 16:41:35,284 >>   Batch size = 4
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:40<00:00,  1.66s/it]***** predict metrics *****
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:42<00:00,  1.75s/it]
  predict_gen_len            =   117.5474
  predict_loss               =     1.7142
  predict_rouge1             =    48.4959
  predict_rouge2             =    22.1124
  predict_rougeL             =    31.1292
  predict_rougeLsum          =    35.9214
  predict_runtime            = 0:00:44.10
  predict_samples            =         95
  predict_samples_per_second =      2.154
  predict_steps_per_second   =      0.544
[INFO|modelcard.py:443] 2022-09-29 16:42:20,740 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Summarization', 'type': 'summarization'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 44.8148}]}
wandb: Waiting for W&B process to finish... (success).
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                   eval/gen_len ▁▁▁▁▁▁▁▁▁▁█
wandb:                      eval/loss █▇▆▅▄▃▂▂▁▁▁
wandb:                    eval/rouge1 ▂▂▂▂▁▁▁▁▁▁█
wandb:                    eval/rouge2 ▂▂▂▂▁▁▂▁▁▂█
wandb:                    eval/rougeL ▂▂▂▂▁▁▂▂▂▂█
wandb:                 eval/rougeLsum ▂▂▂▂▁▁▂▂▂▂█
wandb:                   eval/runtime ▁▁▁▁▁▁▁▁▁▁█
wandb:        eval/samples_per_second ▇█████████▁
wandb:          eval/steps_per_second ▇█████████▁
wandb:                    train/epoch ▁▂▃▃▄▄▅▆▆▇▇███
wandb:              train/global_step ▁▂▃▃▄▄▅▆▆▇▇███
wandb:            train/learning_rate ▁█
wandb:                     train/loss █▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                   eval/gen_len 115.15294
wandb:                      eval/loss 1.76519
wandb:                    eval/rouge1 44.8148
wandb:                    eval/rouge2 17.7844
wandb:                    eval/rougeL 28.0966
wandb:                 eval/rougeLsum 30.9733
wandb:                   eval/runtime 39.4518
wandb:        eval/samples_per_second 2.155
wandb:          eval/steps_per_second 0.558
wandb:                    train/epoch 10.0
wandb:              train/global_step 1120
wandb:            train/learning_rate 0.0
wandb:                     train/loss 3.1809
wandb:               train/total_flos 9251324603006976.0
wandb:               train/train_loss 3.89259
wandb:            train/train_runtime 1571.1378
wandb: train/train_samples_per_second 2.851
wandb:   train/train_steps_per_second 0.713
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /root/wandb/offline-run-20220929_161551-24lxma9o
wandb: Find logs at: ./wandb/offline-run-20220929_161551-24lxma9o/logs
root@4177bb297a48:~# 
